2021-06-28
https://www.linkedin.com/learning/database-clinic-neo4j/creating-a-database?u=61697657

1 neo4j in five mins

2 create a database

2.3 data modelling

	https://www.apcjones.com/arrows/

2.4 creating a datbase

	create (c:County {name: "San Mateo"}) <-[:FOR_COUNTY]-(p:PopulationPrediction
		{population: 1000, year: 2017, gendor: "Male", age: 33, race: "White"})

2.5 import data

	sample csv file: https://goo.gl/8kLTo9

	load csv with headers from "file:///<file.csv>" as row
	return row limit 5

	load csv with headers from "file:///<file.csv>" as row
	return count(*)

	create constraint on (c:County) assert c.code is unique;

	# 스키마 보기 (index, contraints)
	:schema

	# County 노드를 중복없이 임포트
	using periodic commit 500000
	load csv with headers from "file:///<file.csv>" as row
	with distinct row.`County Code` as county_code, row.`County Name` as county_name
	merge (c:County {code: toInteger(county_code)})
	on create set c.name = county_name;

	# 각 County별로 Prediction을 생성하여 연결
	using periodic commit 50000
	load csv with headers from "file:///<file.csv>" as row
	match (c:County {code: toInteger(row.`County Code`)})
	create (po:PoulationPrediction)
	set po.year = toInteger(row.Year),
		po.race = row.`Race name`,
		po.age = toInteger(row.Age),
		po.populatin = toInteger(row.Population)
	create (po)-[:FOR_COUNTY]->(c);

	key takeaway:
		csv 로딩 시 중복 제거하려면?
			use unique constraint
			use merge

2.6 query data

	call db.schema

3. join two datasets

3.1 join datastes

	https://data.gov.uk/dataset/road-accidents-safety-data

	load csv with headers from "file://vehicles_2015.csv" as row
	return row limit 5

		Accident_Index 필드가 조인 키임을 확인

	load csv with headers from "file://Accident_2015.csv" as row
	return row limit 5

		Accident_Severity: "3"	# 이 필드가 계산 타겟

	load csv with headers from "file://Vehicle_Type_Lookup.csv" as row
	return row limit 5

	https://www.apcjones.com/arrows

		Accident - [:INVOLVES] -> Vehicle - [:IS_TYPE]-> VehicleType

		Accident
			accidentIndex: string		# unique
			severity: integer

		VehicleType
			name: string,
			code: string

3.3

3.4
	# 유일 조건
	create constraint on (a:Accident) assert a.accidentIndex is unique;

	# Accident 임포트
	using periodic commit
	load csv with headers from "file:///Accidents_2015.csv" as row
	merge (a:Accident {accidentIndex: row.Accident_Index})
	on create set a.severity = toInteger(row.Accident_Severity);

	# Vehicle 임포트
	using periodic commit
	load csv with headers with "file://Vehicles_2015.csv" as row
	match (a:Accident {accidentIndex: row.Accident_Index})
	create (v:Vehicle)
	# row의 모든 프로퍼티를 v로 로딩하는 문법임
	set v += row
	create (a)-[:INVOLVES]->(v);

	# VehicleType 임포트
	load csv headers from "file://Vehicle_Type_Lookup>csv" as row
	merge (t:VehicleType {code: row.code})
	set t.name = row.label
	with t, row
	match (v:Vehicle) where t.Vehicle_Type = row.code
	create (v)-[:IS_TYPE]->(t);

3.5 compute average accident severity

	#
	call db.schema

	# v1 - 모터사이클 이름 조회
	match (t:VehicleType)
	where t.name contains "Motorcycle"
	return t.name

	# v2 - 모든 모터사이클의 사고 심각도 조회
	match (a:Accident)-[:INVOLVES]->(v:Vehicle)-[:IS_TYPE]->(t:VehicleType)
	where t.name contains "Motorcycle"
	return a.severity

	# v3 - 심각도 평균
	match (a:Accident)-[:INVOLVES]->(v:Vehicle)-[:IS_TYPE]->(t:VehicleType)
	where t.name contains "Motorcycle"
	return avg(a.severity) as avg_severity

	# v4 - group by 구현은 어떻게 하나
	match (a:Accident)-[:INVOLVES]->(v:Vehicle)-[:IS_TYPE]->(t:VehicleType)
	where t.name contains "Motorcycle"
	return a.name, avg(a.severity) as avg_severity

3.6 insights

4. search a database

4.1

4.2

4.3 2014년 CA의 모든 카운티 남, 여 통계를 테이블 형태로.
	#
	call db.schema

	# v1 4.5초 걸림
	match (p:PopulationPrediction)-[:FOR_COUNTY]->(c:County)
	where p.year = 2014 AND p.gender = "Female"
	return c.name, sum(p.population)

	# v2 profile 보기 - NodeByLabelScan -> Expand(All) -> Filter -> ...
	profile match (p:PopulationPrediction)-[:FOR_COUNTY]->(c:County)
	where p.year = 2014 AND p.gender = "Female"
	return c.name, sum(p.population)

	잘 보면 index 대신 많은 rows 자체가 액세스되고 있음을 볼 수 있다.

	# 필터 조건이 Year 이므로 인덱스 생성.
	create index on :PopulationPrediction(year)

	# Index, Constraint 목록 보기
	:schema

	# v3 인덱스 사용했을 때 274ms
	match (p:PopulationPrediction)-[:FOR_COUNTY]->(c:County)
	where p.year = 2014 AND p.gender = "Female"
	return c.name, sum(p.population)

	# v4 profile 보기: 플랜 NodeIndexSeek -> Filter -> ...
	profile match (p:PopulationPrediction)-[:FOR_COUNTY]->(c:County)
	where p.year = 2014 AND p.gender = "Female"
	return c.name, sum(p.population)

4.4 finalize the database query

	# v5 카운티별로 Female, Male 각각 집계 (with를 써서 1차 계산 결과를 그 다음으로 넘긴다)
	match (p:PopulationPrediction)-[:FOR_COUNTY]->(c:County)
	where p.year = 2014 AND p.gender = "Female"
	with c, sum(p.population) as Female
	match (p:PopulationPrediction)-[:FOR_COUNTY]->(c)
	where p.year = 2014 and p.gendor = "Male"
	return c.name as `Year: 2014`, Female, sum(p.population) as Male

5. crud operations

5.1 crud https://www.linkedin.com/learning/database-clinic-neo4j/crud-operations?u=61697657

	neo4j community version

	jupyter notebook

	Character - [:SPEAKS] -> Line

5.3 Connect to Neo4j

		# install driver in jupyter
		!pip install neo4j-driver
		from neo4j.v1 import GraphDatabase
		driver = GraphDatabase.driver("bolt://localhost:7687", auth="neo4j", "letmein")
		with driver.session() as session:
			result = session.run("match (a) return count(a) as num")

			#
			# method 1
			#
			for record in result:
				print(record)

			#
			# method 2
			#
			print(record['num'])

5.4 CREATE

		Create Characters

			with driver.session() as session:
				session.run("create constraint on (c:Character) assert c.name is unique;")
			create_character_query = '''
			merge (c:Character {name: $name})
			'''
			with driver.session() as session:
				with open("data/characters.txt") as f:
					for line in f:
						text = line.strip()
						session.run(create_character_query, parameters={'name': text})


		Create Line nodes

			create_line_query = '''
			create (l:Line {text: $line_text, number: $line_num})
			with l
			match (c:Character) where c.name = $current_character
			create (l)<-[:SPEAKS]-(c)
			'''

			characters = set()

			get_characters_query = '''
			match (c:Character)
			return c.name as name
			'''

			for driver.session as session:
				result = session.run(get_characters_query)
				for record in result:
					characters.add(record['name'])

			print(characters)

		#
		#
		#
		import time

		begin = time.time()
		with driver.session() as session:
			with open("data/A_Midsummer_Nights_Dream.txt") as f:
				current_character = ''
				for line in f:
					text = line.strip()
					if text in characters:
						current_character = text
					else:
						line_num += 1
						session.run(create_line_query, parameterss={
							'line_text': text,
							'line_num': line_num,
							'current_character': current_character
						})
			end = time.time()
			create_time = (end - begin) / line_num * 1000

		print(str(create_time))
		0.360

5.5 UPDATE

		#
		# 대사 가운데 인물명을 검색해봄
		#
		match (l:Line) where l.text contains "LYSANDER"
		return l.text

		# 대사 전체를 대문자로 변환하여 case-insensitive search 해봄
		match (l:Line) where toUpper(l.text) contains "LYSANDER"
		return l.text

		# 이름을 대문자로 치환해봄
		match (l:Line) where toUpper(l.text) contains "LYSANDER"
		return replace(l.text, "Lysander", "LYSANDER")

		# 코드
		update_query = '''
		with $character as character
		with substring(character, 0, 1) + toLower(substring(character, 1)) as casedChar, character
		match (l:Line) where toLower(l.text) contains toLower(character)
		set l.text = replace(l.text, chasedChar, character)
		return count(l) as num
		'''

		update_count = 0

		with driver.session() as session:
			begin = time.time()
			for c in characters:
				result = session.run(update_query, parameters = {'character': c})
				#
				# 리턴값이 싱글 레코드일 때
				#
				record = result.single()
				update_count += record['num']
			end = time.time()
			update = (end - begin) / update_count * 1000
			print(update_time)
			0.635

		# 다시 검색해봄. 모두 대문자로 변환되었나?
		match (l:Line) where l.text contains "LYSANDER"
		return l.text

5.6 DELETE

		#
		delete_query = '''
		with ['ENTER', 'EXIST', 'ACT', 'SCENE'] as words
		match (l:Line) where any(x in words where toLower(l.text) starts with toLower(x))
		detach delete l
		return count(l) as num
		'''

		#
		with driver.session() as session:
			begin = time.time()
			result = session.run(delete_query)
			record = result.single()
			num_lines = record['num']
			end = time.time()
			delete_time = (end - begin) / num_lines * 1000
			print(delete_time)

5.7 READ

	#
	read_query = '''
	match (l:Line) return l.text as text, l.number as linenum
	'''

	#
	with driver.session() as session
		begin = time.time()
		result = session.run(read_query)
		for record in result:

			# Line 출력
			print("%i: %s" % (record['linenum'], record['text']))

		# 라인수 출력
		result = session.run("match (l:Line) return coutn(l) as num")
		numlines = result.single()['num']

		end = time.time()
		read_time = (end - begin) / num_lines * 1000
		print(str(read_time))


6. averages and calculations

6.2
	# year, population을 data set 1으로부터 추출

	# education level, percent를 data set 2로부터 추출

	# year, educatino level, forecast를 생성

6.3 update the data model

	Year {year} <- PopulationPrediction {population, year, age, race, gender} -[:FOR_COUNTY]-> County {code, name}

	EducationObservation {population, year, income, age, education, gender} -[:FOR_YEAR]-> Year {year}

6.4
	# Year 노드 먼저 생성
	create constraint on (y:Year) assert y.year is unique

	# PP 노드에서 year를 모두 추출. 아직 연결된 건 아님.
	match (p:PopulationPrediction)
	with distinct p.year as pop_year
	mege (y:Year {year: pop_year});

	# 이거 동작하기는 하겠지만 tx 볼륨이 너무 큼.
	MATCH (y:Year)
	MATCH (p:PopulationPrediction) WHERE p.year = y.year
	CREATE (p)-[:FOR_YEAR]->(y)

	# tx를 작은 chunk로 자르기 위해, 자바 코드를 cypher에서 실행하는 거 가능함.

	# apoc 설치. jar 파일을 plugins에 넣고 db restart.

	# 확인
	call dbms.procedures

	# apoc을 사용하여 쪼개서 임포트
	call apoc.periodic.iterate("match (y:Year) return y", "match (p:PopulationPrediction) where p.year  = y.year create (p)-[:FOR_YEAR]->(y)", {batchSize: 1, parallel: true, iterateList: true})

	# 실행 에러
	ERROR: Neo.ClientError.Procedure.ProcedureRegistrationFailed

	# 다시 실행. 몇분 걸림. 7밀리언 레코드 처리.
	call apoc.periodic.iterate("match (y:Year) return y", "match (p:PopulationPrediction) where p.year  = y.year create (p)-[:FOR_YEAR]->(y)", {batchSize: 1, parallel: true, iterateList: true})

	#
	call db.schema

	County <-[:FOR_COUNTY]- PP -[:FOR_YEAR] -> Year

6.5 Import data for calculatins

	# 이제 Educatinoal Attainment 데이터 임포트할 차례

	# csv 파일 구조 보기
	load csv with headers from "file://a.csv" as row
	return row limit 5

	# Year 필드가 01/01/2008 12:00:00:AM 포맷으로 저장되어 있음을 볼 수 있다.

	#
	load csv with headers from "file://a.csv" as row
	return apoc.date.fields(row.Year, 'MM/dd/yyyy hh:mm:ss a') limit 5

	각 레코드가 date 오브젝에 파싱되어 있다
	{
		"weekdays": 2,
		"years": 2008,	<== 이거 참조할 예정
		"seconds": 0,
		"zoneid": "UTC",
		"minutes": 0,
		"hours": 0,
		"months": 1,
		"days": 1
	}

	# with로 전환하고, 기존 year에다가 새 노드 생성 후 연결.
	load csv with headers from "file://a.csv" as row
	with row, apoc.date.fields(row.Year, 'MM/dd/yyyy hh:mm:ss a')['years'] as parsed_year
	match (y:Year) where y.year = parsed_year
	create (e:EducationObservation)
	set e.population = toInteger(row.`PopulationCount`),
	    e.year = parsed_year,
		e.income = row.`Personal Income`,
		e.age = e.Age,
		e.education = row.`Educational Attainment`,
		e.gender = row.Gender
	create (y)<-[:FOR_YEAR]-(e)

	# 이제 생성된 모델은 아래와 같다

	County <-- PopulatinoPrediction -> Year <- EducationObservation

6.6 perform calculation

